{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c257b7",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28969d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install datasets -q\\n!pip install transformers -q\\nimport sys\\n!{sys.executable} -m pip install faiss-gpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!pip install datasets -q\n",
    "!pip install transformers -q\n",
    "import sys\n",
    "!{sys.executable} -m pip install faiss-gpu\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cfce8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79602712",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 256\n",
    "MAX_LENGTH = 512\n",
    "AMP_DTYPE = torch.float32\n",
    "K_VALUES = 5\n",
    "BATCH_Q = 32  # query batch size\n",
    "EMB_OUTPUT_PATH = Path(\"REG/v1/data/embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada3c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"nvidia/TechQA-RAG-Eval\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-large-en-v1.5\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e416633",
   "metadata": {},
   "source": [
    "# 1. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8589b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(text: str) -> torch.Tensor:\n",
    "    if isinstance(text, str):\n",
    "        prefixed = [f\"query: {text}\"]  # BGE recommendation for queries\n",
    "    else:\n",
    "        prefixed = [f\"query: {q}\" for q in text]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            prefixed,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", dtype=AMP_DTYPE):\n",
    "            emb = model(**inputs).last_hidden_state[:, 0, :]  \n",
    "    return F.normalize(emb.squeeze(0), p=2, dim=-1).to(torch.float32)\n",
    "\n",
    "def assert_1_to_1(ds, splits=None):\n",
    "    filename_to_text, text_to_filename = {}, {}\n",
    "    splits = splits or list(ds.keys())\n",
    "\n",
    "    for split in splits:\n",
    "        dsplit = ds[split]\n",
    "        for row in dsplit:\n",
    "            for ctx in row[\"contexts\"]:\n",
    "                fname = ctx[\"filename\"]\n",
    "                text = ctx[\"text\"].strip()\n",
    "\n",
    "                if fname in filename_to_text:\n",
    "                    assert filename_to_text[fname] == text\n",
    "                else:\n",
    "                    filename_to_text[fname] = text\n",
    "\n",
    "                if text in text_to_filename:\n",
    "                    assert text_to_filename[text] == fname\n",
    "                else:\n",
    "                    text_to_filename[text] = fname\n",
    "                    \n",
    "def load_corpus(ds, splits=None):\n",
    "    \"\"\"Return list of unique context texts; index = implicit doc_id.\"\"\"\n",
    "    assert_1_to_1(ds, splits)\n",
    "    splits = splits or list(ds.keys())\n",
    "    texts = []\n",
    "    fname_to_idx = {}\n",
    "    seen_filenames = set()\n",
    "    for split in splits:\n",
    "        for row in ds[split]:\n",
    "            for ctx in row[\"contexts\"]:\n",
    "                fname = ctx[\"filename\"]\n",
    "                if fname in seen_filenames:\n",
    "                    continue\n",
    "                seen_filenames.add(fname)\n",
    "                fname_to_idx[fname] = len(texts)\n",
    "                texts.append(ctx[\"text\"].strip())\n",
    "    return texts, fname_to_idx\n",
    "\n",
    "def store_ctx_embs():\n",
    "    texts, _ = load_corpus(ds)\n",
    "    num_docs = len(texts)\n",
    "    print(f\"Loaded {num_docs} passages\")\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    emb_dim = model.pooler.dense.out_features\n",
    "    print(f\"Embedding dimension: {emb_dim}\")\n",
    "    all_embs = torch.zeros((num_docs, emb_dim))\n",
    "    num_batches = math.ceil(num_docs / BATCH_SIZE)\n",
    "    print(f\"Encoding in {num_batches} batches (batch_size={BATCH_SIZE})\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for b in tqdm(range(num_batches), desc=\"Embedding corpus\"):\n",
    "            start, end = b * BATCH_SIZE, min((b + 1) * BATCH_SIZE, num_docs)\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                [f\"passage: {t}\" for t in texts[start:end]],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", dtype=AMP_DTYPE):\n",
    "                outputs = model(**inputs).last_hidden_state[:, 0, :]  \n",
    "            all_embs[start:end, :] = F.normalize(outputs.detach(), p=2, dim=-1)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    docs_per_sec = num_docs / total_time if total_time > 0 else float(\"inf\")\n",
    "\n",
    "    print(f\"Embedded {num_docs} passages in {total_time:.2f} s \"\n",
    "            f\"({docs_per_sec:.1f} passages/s)\")\n",
    "\n",
    "    all_embs = all_embs.contiguous().cpu().to(torch.float32)\n",
    "    EMB_OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(all_embs, EMB_OUTPUT_PATH)\n",
    "    print(f\"Saved embeddings to {EMB_OUTPUT_PATH}\")\n",
    "\n",
    "#f()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c109e8",
   "metadata": {},
   "source": [
    "# 2. Pre-Process Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa30014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 496 unique passages\n",
      "Evaluating retrieval on 910 TechQA examples (batched)...\n"
     ]
    }
   ],
   "source": [
    "# 1) Build corpus using our single canonical helper\n",
    "texts, fname_to_idx = load_corpus(ds)   \n",
    "num_docs = len(texts)\n",
    "print(f\"Corpus has {num_docs} unique passages\")\n",
    "\n",
    "questions = []\n",
    "gold_ids_per_query = []\n",
    "\n",
    "for row in ds[\"train\"]:\n",
    "    question = row[\"question\"]\n",
    "    contexts = row[\"contexts\"]\n",
    "    gold_doc_ids = []\n",
    "    for ctx in contexts:\n",
    "        fname = ctx[\"filename\"].strip()\n",
    "        doc_id = fname_to_idx.get(fname, None)\n",
    "        if doc_id is not None:\n",
    "            gold_doc_ids.append(doc_id)\n",
    "\n",
    "    questions.append(question)\n",
    "    gold_ids_per_query.append(gold_doc_ids)\n",
    "\n",
    "total = len(questions)\n",
    "print(f\"Evaluating retrieval on {total} TechQA examples (batched)...\")\n",
    "\n",
    "embs = torch.load(EMB_OUTPUT_PATH).to(DEVICE)\n",
    "N = embs.shape[0]          \n",
    "total = len(questions)     \n",
    "gold_mask = torch.zeros(total, N, dtype=torch.bool, device=DEVICE)\n",
    "for i, gold_ids in enumerate(gold_ids_per_query):\n",
    "    if not gold_ids:\n",
    "        continue\n",
    "    gold_mask[i, gold_ids] = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0a7db",
   "metadata": {},
   "source": [
    "# 3. Q Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53163035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Hit@K batched: 100%|██████████| 29/29 [00:12<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hit@5 (batched): 553/910 = 0.608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hits = 0\n",
    "for start in tqdm(range(0, total, BATCH_Q), desc=\"Evaluating Hit@K batched\"):\n",
    "    end = min(start + BATCH_Q, total)\n",
    "    batch_q = questions[start:end]\n",
    "    q_embs = embed_query(batch_q)  # [B, dim]\n",
    "\n",
    "    sims = q_embs @ embs.T                  # [B, N]\n",
    "    _, top_idx = sims.topk(K_VALUES, dim=1)        # [B, K]\n",
    "\n",
    "    batch_gold_mask = gold_mask[start:end]             # [B, N]\n",
    "    retrieved_mask = torch.zeros_like(batch_gold_mask) # [B, N]\n",
    "    retrieved_mask.scatter_(1, top_idx, True)\n",
    "    batch_hits = (batch_gold_mask & retrieved_mask).any(dim=1)  # [B]\n",
    "    hits += batch_hits.sum().item()\n",
    "\n",
    "hit_at_k = hits / total\n",
    "print(f\"\\nHit@{K_VALUES} (batched): {hits}/{total} = {hit_at_k:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811a697c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id=0, score=0.790\n",
      "Title: IBM STREAMS 4.1.1.1 and 4.1.1.2  JOBS DO NOT INHERIT THE ENVIRONMENT VARIABLES SET IN .BASHRC, WHEN STREAMS IS RUN AS A SYSTEM SERVICE - United States  Text:  FLASH (ALERT)  ABSTRACT  In Stream...\n",
      "\n",
      "doc_id=239, score=0.661\n",
      "Title: IBM ERROR: Default DB path is not set, when adding database set on RHEL 5 - United States  Text: 1320206; ClearQuest; Command Line Tools; CQ; cqreg; add_dbset; initialize; CQ_DATABASES; CQDB_rg...\n",
      "\n",
      "doc_id=235, score=0.649\n",
      "Title: IBM Data Server Manager (DSM) showing SQLCODE=-206 \"<name> is not valid in the context where it is used.\" - United States  Text: SQLCODE 206 -206 SQL0206 SQL0206N DSM incompatible db2level fixp...\n",
      "\n",
      "doc_id=99, score=0.645\n",
      "Title: IBM web server Plugin may need LD_LIBRARY_PATH when used with Apache 2.2 - United States  Text:  TECHNOTE (TROUBLESHOOTING)  PROBLEM(ABSTRACT)  IBM web server Plug-in provides the connection be...\n",
      "\n",
      "doc_id=351, score=0.644\n",
      "Title: IBM Known Issues for DB2 on Linux - United States  Text:  TECHNOTE (FAQ)  QUESTION  What are known issues on Linux platforms related to DB2® database products ?   ANSWER Known Issues with DB2 o...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve(question: str):\n",
    "    \"\"\"\n",
    "    RAG retriever: question -> top-k (score, doc_id, text) from corpus.\n",
    "    \"\"\"\n",
    "    q_emb = embed_query(question)            # [dim] on device\n",
    "    sims = embs @ q_emb                      # [N]\n",
    "    top_vals, top_idx = torch.topk(sims, k=K_VALUES)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(top_vals.tolist(), top_idx.tolist()):\n",
    "        results.append(\n",
    "            {\n",
    "                \"doc_id\": idx,\n",
    "                \"score\": float(score),\n",
    "                \"text\": texts[idx],\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "q = ds[\"train\"][0][\"question\"]\n",
    "ret = retrieve(q)\n",
    "for r in ret:\n",
    "    print(f\"doc_id={r['doc_id']}, score={r['score']:.3f}\")\n",
    "    print(r[\"text\"][:200].replace(\"\\n\", \" \") + \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf3a155",
   "metadata": {},
   "source": [
    "# 4. LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb7a1916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edab0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gwen():\n",
    "\tmessages = [\n",
    "\t\t{\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "\t]\n",
    "\tinputs = qwen_tokenizer.apply_chat_template(\n",
    "\t\tmessages,\n",
    "\t\tadd_generation_prompt=True,\n",
    "\t\ttokenize=True,\n",
    "\t\treturn_dict=True,\n",
    "\t\treturn_tensors=\"pt\",\n",
    "\t).to(qwen_model.device)\n",
    "\n",
    "\toutputs = qwen_model.generate(**inputs, max_new_tokens=40)\n",
    "\tprint(qwen_tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "064c361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_contexts(retrieved):\n",
    "    chunks = []\n",
    "    for i, r in enumerate(retrieved, start=1):\n",
    "        text = r[\"text\"]\n",
    "        chunks.append(f\"[DOC {i}] (score={r['score']:.3f})\\n{text}\")\n",
    "    return \"\\n\\n\".join(chunks)\n",
    "\n",
    "\n",
    "def build_rag_messages(question, retrieved):\n",
    "    context_block = format_contexts(retrieved)\n",
    "    system_prompt = (\n",
    "        \"You are a helpful technical support assistant. \"\n",
    "        \"Explain the root cause briefly and then give the concrete workaround or commands \"\n",
    "        \"the user should run, based ONLY on the documents. \"\n",
    "        \"If the documents do not contain a clear solution, say you are not sure.\"\n",
    "    )\n",
    "    user_content = f\"\"\"Question:\n",
    "    {question}\n",
    "\n",
    "    Relevant documents:\n",
    "    {context_block}\n",
    "\n",
    "    Answer the question in your own words. If you use a document, mention its DOC number.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": user_content},\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def call_llm_qwen(messages, max_new_tokens=256, temperature=0.2, top_p=0.9):\n",
    "    inputs = qwen_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(qwen_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = qwen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "\n",
    "    gen_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    answer = qwen_tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "    return answer\n",
    "\n",
    "def answer_question(question: str):\n",
    "    retrieved = retrieve(question) \n",
    "    messages = build_rag_messages(question, retrieved)\n",
    "    answer = call_llm_qwen(messages)\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": retrieved,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08a9d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " The root cause of the issue is that in IBM Streams 4.1.1.1 and 4.1.1.2, when Streams is run as a system service, environment variables set in the `.bashrc` file are not being inherited by the Streams jobs. This is different from earlier releases.\n",
      "\n",
      "To work around this issue, you need to set the required environment variables directly in the Streams instance using `streamtool`. Specifically, you should use the following command:\n",
      "\n",
      "```sh\n",
      "streamtool setproperty -d <domain> -i <instance> --application-env <VARIABLE_NAME>=<VARIABLE_VALUE>\n",
      "```\n",
      "\n",
      "Replace `<domain>` with the name of your domain, `<instance>` with the name of your instance, `<VARIABLE_NAME>` with the name of the environment variable you want to set, and `<VARIABLE_VALUE>` with the value of the environment variable.\n",
      "\n",
      "For example, if you need to set the `ODBCINI` variable, you would run:\n",
      "\n",
      "```sh\n",
      "streamtool setproperty -d mydomain -i myinstance --application-env ODBCINI=/path/to/odbc.ini\n",
      "```\n",
      "\n",
      "This will ensure that the necessary environment variables are set for your Streams jobs when they are run as a system service. \n",
      "\n",
      "DOCS USED:\n",
      "- doc_id=0, score=0.790\n",
      "- doc_id=239, score=0.661\n",
      "- doc_id=235, score=0.649\n",
      "- doc_id=99, score=0.645\n",
      "- doc_id=351, score=0.644\n"
     ]
    }
   ],
   "source": [
    "q = ds[\"train\"][0][\"question\"]\n",
    "res = answer_question(q)\n",
    "print(\"ANSWER:\\n\", res[\"answer\"], \"\\n\")\n",
    "print(\"DOCS USED:\")\n",
    "for c in res[\"contexts\"]:\n",
    "    print(f\"- doc_id={c['doc_id']}, score={c['score']:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.7.1 (cu118)",
   "language": "python",
   "name": "pt27cu118"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
